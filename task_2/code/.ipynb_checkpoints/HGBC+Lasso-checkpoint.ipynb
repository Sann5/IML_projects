{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2, Subtask 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "import sklearn.metrics as metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We read the already normalized and imputed data. For specifics about the imputation and normalization \n",
    "## see imputate.R file. \n",
    "test_feat_path = \"../data/test_features_imp.csv\" \n",
    "train_feat_path = \"../data/train_features_imp.csv\" \n",
    "train_lab_path = \"../data/train_labels.csv\"\n",
    "test_feat = pd.read_csv(test_feat_path)\n",
    "train_feat = pd.read_csv(train_feat_path)\n",
    "train_lab = pd.read_csv(train_lab_path)\n",
    "\n",
    "## Order data to make sure that rows in X and Y match\n",
    "test_feat.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "train_feat.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "train_lab.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "\n",
    "## Select exclude the pid column and make into array\n",
    "X_test = test_feat.iloc[:, 1:272].values\n",
    "X_train = train_feat.iloc[:, 1:272].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1\n",
    "### Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC validation and training score (t.s on probabilities), for each label:\n",
      "0.9373635209479475   0.9753031880702936\n",
      "0.7549797077922078   0.9026598465473147\n",
      "0.7412113646304126   0.852573851830949\n",
      "0.7613488278569883   0.8731191490363147\n",
      "0.73456405403889   0.8339599224844418\n",
      "0.8192191828254848   0.8619365002020938\n",
      "0.8664142813173283   0.9300488203798855\n",
      "0.8372654935154936   0.9118233959642607\n",
      "0.7361366421568627   0.9384280088989346\n",
      "0.9490929577464788   0.9949929011335982\n"
     ]
    }
   ],
   "source": [
    "## Define the names of the labels to predict\n",
    "labels_subtask_1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST',\n",
    "                    'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', \n",
    "                    'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "                    'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "## Write to an array the labels of interest\n",
    "Y_train = train_lab[labels_subtask_1].to_numpy()\n",
    "\n",
    "## Make empty df to fill with the results\n",
    "output = pd.DataFrame({'pid': test_feat.iloc[:, 0].values})\n",
    "\n",
    "## For every label in Y_train fit a HGBC and use it to predict the probabilities of X_test\n",
    "print(\"ROC AUC validation and training score (training score on probability estimates), for each label:\")\n",
    "for i, label in enumerate(labels_subtask_1):\n",
    "    ## Fit model\n",
    "    clf = HistGradientBoostingClassifier(scoring = 'roc_auc', \n",
    "                                         random_state = 123).fit(X_train, Y_train[:, i])\n",
    "    \n",
    "    ## Print the testing and traing score. Trainig score is estimated for the probability estimates not the labels.\n",
    "    print(clf.validation_score_[np.size(clf.validation_score_) - 1],\n",
    "          \" \", \n",
    "          metrics.roc_auc_score(Y_train[:, i],\n",
    "          clf.predict_proba(X_train)[:, 1], average='micro'))\n",
    "    \n",
    "    ## Write to results df\n",
    "    output[label] = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2\n",
    "### Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC validation and training score (t.s on probabilities), for each label:\n",
      "0.7436315112770785   0.952782749680539\n"
     ]
    }
   ],
   "source": [
    "## Write to an array the labels of interest\n",
    "Y_train = train_lab['LABEL_Sepsis'].to_numpy()\n",
    "\n",
    "## Fit a HGBC and use it to predict the probabilities of X_test\n",
    "print(\"ROC AUC validation and training score (training score on probability estimates), for each label:\")\n",
    "\n",
    "## Fit model\n",
    "clf = HistGradientBoostingClassifier(scoring = 'roc_auc',\n",
    "                                     random_state = 123).fit(X_train, Y_train)\n",
    "    \n",
    "## Print the testing and traing score. Trainig score is estimated for the probability estimates not the labels.\n",
    "print(clf.validation_score_[np.size(clf.validation_score_) - 1],\n",
    "      \" \",\n",
    "      metrics.roc_auc_score(Y_train,\n",
    "                            clf.predict_proba(X_train)[:, 1],\n",
    "                            average='micro'))\n",
    "\n",
    "## Write to results df\n",
    "output['LABEL_Sepsis'] = clf.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3\n",
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training scores for each label:\n",
      "0.37770345083252754\n",
      "0.5859785441608802\n",
      "0.38386074780919743\n",
      "0.6144724385873669\n"
     ]
    }
   ],
   "source": [
    "## Define the features to predict for this rask\n",
    "labels_subtask_3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "\n",
    "## Write to an array the labels of interest\n",
    "Y_train = train_lab[labels_subtask_3].to_numpy()\n",
    "\n",
    "## Fit Lasso regression to the data and predict\n",
    "print(\"Training scores for each label:\")\n",
    "for i, label in enumerate(labels_subtask_3):\n",
    "    ## Get suffix of the label to predict\n",
    "    sufix = label.split(\"_\", maxsplit = 2)[1] + \"$\"\n",
    "    \n",
    "    ## Filter out columns that dont end with the suffix\n",
    "    X_in_loop_train = train_feat.filter(regex = sufix, axis = 1).to_numpy()\n",
    "    X_in_loop_test = test_feat.filter(regex = sufix, axis = 1).to_numpy()\n",
    "    \n",
    "    ## Fit model\n",
    "    reg = LassoCV(random_state = 123, \n",
    "                  verbose = False,\n",
    "                  max_iter = 10000).fit(X_in_loop_train, Y_train[:, i])\n",
    "    \n",
    "    ## Print training score (the suck)\n",
    "    print(reg.score(X_in_loop_train, Y_train[:, i]))\n",
    "    \n",
    "    ## Write to output\n",
    "    output[label] = reg.predict(X_in_loop_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pid</th>\n",
       "      <th>LABEL_BaseExcess</th>\n",
       "      <th>LABEL_Fibrinogen</th>\n",
       "      <th>LABEL_AST</th>\n",
       "      <th>LABEL_Alkalinephos</th>\n",
       "      <th>LABEL_Bilirubin_total</th>\n",
       "      <th>LABEL_Lactate</th>\n",
       "      <th>LABEL_TroponinI</th>\n",
       "      <th>LABEL_SaO2</th>\n",
       "      <th>LABEL_Bilirubin_direct</th>\n",
       "      <th>LABEL_EtCO2</th>\n",
       "      <th>LABEL_Sepsis</th>\n",
       "      <th>LABEL_RRate</th>\n",
       "      <th>LABEL_ABPm</th>\n",
       "      <th>LABEL_SpO2</th>\n",
       "      <th>LABEL_Heartrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.914906</td>\n",
       "      <td>0.513746</td>\n",
       "      <td>0.842769</td>\n",
       "      <td>0.819782</td>\n",
       "      <td>0.790402</td>\n",
       "      <td>0.491066</td>\n",
       "      <td>0.075179</td>\n",
       "      <td>0.297925</td>\n",
       "      <td>0.040125</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.128961</td>\n",
       "      <td>15.092701</td>\n",
       "      <td>81.921415</td>\n",
       "      <td>98.225746</td>\n",
       "      <td>84.676908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.060852</td>\n",
       "      <td>0.246728</td>\n",
       "      <td>0.205171</td>\n",
       "      <td>0.136316</td>\n",
       "      <td>0.055908</td>\n",
       "      <td>0.042676</td>\n",
       "      <td>0.059464</td>\n",
       "      <td>0.021740</td>\n",
       "      <td>0.012837</td>\n",
       "      <td>0.019549</td>\n",
       "      <td>17.708045</td>\n",
       "      <td>85.290360</td>\n",
       "      <td>96.737910</td>\n",
       "      <td>95.107560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>0.033072</td>\n",
       "      <td>0.046898</td>\n",
       "      <td>0.128559</td>\n",
       "      <td>0.179086</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>0.068252</td>\n",
       "      <td>0.067896</td>\n",
       "      <td>0.058745</td>\n",
       "      <td>0.022675</td>\n",
       "      <td>0.022711</td>\n",
       "      <td>0.030466</td>\n",
       "      <td>19.016770</td>\n",
       "      <td>72.983975</td>\n",
       "      <td>95.905009</td>\n",
       "      <td>71.068876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>0.844867</td>\n",
       "      <td>0.815086</td>\n",
       "      <td>0.897217</td>\n",
       "      <td>0.960052</td>\n",
       "      <td>0.910483</td>\n",
       "      <td>0.468831</td>\n",
       "      <td>0.061377</td>\n",
       "      <td>0.496812</td>\n",
       "      <td>0.340772</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>0.075632</td>\n",
       "      <td>17.501360</td>\n",
       "      <td>86.498119</td>\n",
       "      <td>98.069292</td>\n",
       "      <td>94.273838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>0.086911</td>\n",
       "      <td>0.073689</td>\n",
       "      <td>0.252036</td>\n",
       "      <td>0.277729</td>\n",
       "      <td>0.287920</td>\n",
       "      <td>0.096186</td>\n",
       "      <td>0.073415</td>\n",
       "      <td>0.069117</td>\n",
       "      <td>0.014736</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.031630</td>\n",
       "      <td>20.007447</td>\n",
       "      <td>87.643147</td>\n",
       "      <td>95.843105</td>\n",
       "      <td>91.415635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12659</th>\n",
       "      <td>31647</td>\n",
       "      <td>0.026496</td>\n",
       "      <td>0.062178</td>\n",
       "      <td>0.141386</td>\n",
       "      <td>0.170821</td>\n",
       "      <td>0.126407</td>\n",
       "      <td>0.065149</td>\n",
       "      <td>0.067896</td>\n",
       "      <td>0.050096</td>\n",
       "      <td>0.015539</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>17.229005</td>\n",
       "      <td>69.264997</td>\n",
       "      <td>96.635856</td>\n",
       "      <td>74.800623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12660</th>\n",
       "      <td>31649</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.070331</td>\n",
       "      <td>0.405186</td>\n",
       "      <td>0.380808</td>\n",
       "      <td>0.375513</td>\n",
       "      <td>0.684463</td>\n",
       "      <td>0.042917</td>\n",
       "      <td>0.241261</td>\n",
       "      <td>0.022399</td>\n",
       "      <td>0.005445</td>\n",
       "      <td>0.043661</td>\n",
       "      <td>16.076435</td>\n",
       "      <td>82.614546</td>\n",
       "      <td>97.025402</td>\n",
       "      <td>89.095196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12661</th>\n",
       "      <td>31651</td>\n",
       "      <td>0.599231</td>\n",
       "      <td>0.045610</td>\n",
       "      <td>0.213162</td>\n",
       "      <td>0.183221</td>\n",
       "      <td>0.167425</td>\n",
       "      <td>0.132979</td>\n",
       "      <td>0.069538</td>\n",
       "      <td>0.302960</td>\n",
       "      <td>0.013578</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.063752</td>\n",
       "      <td>17.701808</td>\n",
       "      <td>79.185616</td>\n",
       "      <td>98.225746</td>\n",
       "      <td>82.267047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12662</th>\n",
       "      <td>31652</td>\n",
       "      <td>0.010771</td>\n",
       "      <td>0.045973</td>\n",
       "      <td>0.231386</td>\n",
       "      <td>0.202682</td>\n",
       "      <td>0.264693</td>\n",
       "      <td>0.062295</td>\n",
       "      <td>0.105402</td>\n",
       "      <td>0.031817</td>\n",
       "      <td>0.016451</td>\n",
       "      <td>0.049580</td>\n",
       "      <td>0.024161</td>\n",
       "      <td>19.294190</td>\n",
       "      <td>94.665188</td>\n",
       "      <td>97.395372</td>\n",
       "      <td>116.851730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12663</th>\n",
       "      <td>31655</td>\n",
       "      <td>0.038998</td>\n",
       "      <td>0.107205</td>\n",
       "      <td>0.263377</td>\n",
       "      <td>0.200347</td>\n",
       "      <td>0.206094</td>\n",
       "      <td>0.102689</td>\n",
       "      <td>0.591748</td>\n",
       "      <td>0.147497</td>\n",
       "      <td>0.028919</td>\n",
       "      <td>0.020054</td>\n",
       "      <td>0.044556</td>\n",
       "      <td>18.297481</td>\n",
       "      <td>84.234870</td>\n",
       "      <td>98.225746</td>\n",
       "      <td>106.372173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12664 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pid  LABEL_BaseExcess  LABEL_Fibrinogen  LABEL_AST  \\\n",
       "0          0          0.914906          0.513746   0.842769   \n",
       "1          3          0.010101          0.060852   0.246728   \n",
       "2          5          0.033072          0.046898   0.128559   \n",
       "3          7          0.844867          0.815086   0.897217   \n",
       "4          9          0.086911          0.073689   0.252036   \n",
       "...      ...               ...               ...        ...   \n",
       "12659  31647          0.026496          0.062178   0.141386   \n",
       "12660  31649          0.798826          0.070331   0.405186   \n",
       "12661  31651          0.599231          0.045610   0.213162   \n",
       "12662  31652          0.010771          0.045973   0.231386   \n",
       "12663  31655          0.038998          0.107205   0.263377   \n",
       "\n",
       "       LABEL_Alkalinephos  LABEL_Bilirubin_total  LABEL_Lactate  \\\n",
       "0                0.819782               0.790402       0.491066   \n",
       "1                0.205171               0.136316       0.055908   \n",
       "2                0.179086               0.133181       0.068252   \n",
       "3                0.960052               0.910483       0.468831   \n",
       "4                0.277729               0.287920       0.096186   \n",
       "...                   ...                    ...            ...   \n",
       "12659            0.170821               0.126407       0.065149   \n",
       "12660            0.380808               0.375513       0.684463   \n",
       "12661            0.183221               0.167425       0.132979   \n",
       "12662            0.202682               0.264693       0.062295   \n",
       "12663            0.200347               0.206094       0.102689   \n",
       "\n",
       "       LABEL_TroponinI  LABEL_SaO2  LABEL_Bilirubin_direct  LABEL_EtCO2  \\\n",
       "0             0.075179    0.297925                0.040125     0.002910   \n",
       "1             0.042676    0.059464                0.021740     0.012837   \n",
       "2             0.067896    0.058745                0.022675     0.022711   \n",
       "3             0.061377    0.496812                0.340772     0.011140   \n",
       "4             0.073415    0.069117                0.014736     0.000683   \n",
       "...                ...         ...                     ...          ...   \n",
       "12659         0.067896    0.050096                0.015539     0.001867   \n",
       "12660         0.042917    0.241261                0.022399     0.005445   \n",
       "12661         0.069538    0.302960                0.013578     0.001275   \n",
       "12662         0.105402    0.031817                0.016451     0.049580   \n",
       "12663         0.591748    0.147497                0.028919     0.020054   \n",
       "\n",
       "       LABEL_Sepsis  LABEL_RRate  LABEL_ABPm  LABEL_SpO2  LABEL_Heartrate  \n",
       "0          0.128961    15.092701   81.921415   98.225746        84.676908  \n",
       "1          0.019549    17.708045   85.290360   96.737910        95.107560  \n",
       "2          0.030466    19.016770   72.983975   95.905009        71.068876  \n",
       "3          0.075632    17.501360   86.498119   98.069292        94.273838  \n",
       "4          0.031630    20.007447   87.643147   95.843105        91.415635  \n",
       "...             ...          ...         ...         ...              ...  \n",
       "12659      0.015392    17.229005   69.264997   96.635856        74.800623  \n",
       "12660      0.043661    16.076435   82.614546   97.025402        89.095196  \n",
       "12661      0.063752    17.701808   79.185616   98.225746        82.267047  \n",
       "12662      0.024161    19.294190   94.665188   97.395372       116.851730  \n",
       "12663      0.044556    18.297481   84.234870   98.225746       106.372173  \n",
       "\n",
       "[12664 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visualize output\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write results to .zip\n",
    "output.to_csv('../output/submission_HGBC_Lasso.zip', index=False, float_format='%.3f', compression='zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.6583995858828227 minutes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s minutes ---\" % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1. Binary Relevance and HGBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | C | kernel | gamma | weight | features | n_features | F1 score | AUC | runtime (min) |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| run_1 |  1 |  rbf | scale  |  balanced |  median for NA's and mean  | 35 | 0.598165656150447 | ? | 33 |\n",
    "| run_2 |  1 |  rbf | scale  |  balanced |  median for NA's and mean, max, min, median, sd  | 170 | 0.628216870267411 |?| 102 |\n",
    "| run_3 |  1 |  rbf | scale  |  balanced |  median for NA's and mean, max, min, median, sd, range, skw, kurt  | 272 | 0.649372121402984 | 0.8236937992110356 | 141 |\n",
    "| run_4 |  HGBC |  HGBC | HGBC |  HGBC |  median for NA's and mean, max, min, median, sd, range, skw, kurt  | 272 | 0.871097657278231* | 0.8222653647930391 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I think the reason for this high score is beacuse the f1_micro is more severe when all labels are taken into account instead of one by one and the averaging. Hence I dont belive the HGBC is superiro in terms of performance, otherwise we would have also observed a big increase in the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3. Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainig scores for normalized and unnormalized imputed data restircted to the labels:\n",
    "\n",
    "|nomralized|UN-nomralized|\n",
    "|---|---|\n",
    "|0.37770345083252754 | 0.37759566055685045|\n",
    "|0.5859785441608802  | 0.5856645886174903 |\n",
    "|0.38386074780919743 | 0.3842306307116389 |\n",
    "|0.6144724385873669  | 0.6142282361433877 | \n",
    "\n",
    "The sumbission scores were only a little bit different for normalized and unormalized data. 0.754641671097 and 0.754664968318 respectively. We therofre decide to use normalized data becasue this way we dont need two imputation scripts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
