{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2, Subtask 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We read the already normalized and imputed data\n",
    "test_feat_path = \"../data/test_features_imp.csv\" \n",
    "train_feat_path = \"../data/train_features_imp.csv\" \n",
    "train_lab_path = \"../data/train_labels.csv\"\n",
    "\n",
    "test_feat = pd.read_csv(test_feat_path)\n",
    "train_feat = pd.read_csv(train_feat_path)\n",
    "train_lab = pd.read_csv(train_lab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Order data to make sure that rows in X and Y match\n",
    "test_feat.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "train_feat.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "train_lab.sort_values(by=['pid'], inplace = True, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select relevant label columns and exclude the pid column\n",
    "X_test = train_feat.iloc[:, 1:34].values\n",
    "X_train = train_feat.iloc[:, 1:34].values\n",
    "train_lab = train_lab[[\"LABEL_BaseExcess\", \"LABEL_Fibrinogen\",\n",
    "                   \"LABEL_AST\", \"LABEL_Alkalinephos\",\n",
    "                   \"LABEL_Bilirubin_total\", \"LABEL_Lactate\",\n",
    "                   \"LABEL_TroponinI\", \"LABEL_SaO2\",\n",
    "                   \"LABEL_Bilirubin_direct\", \"LABEL_EtCO2\"]]\n",
    "Y_train = train_lab.iloc[:,:].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train binary relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Binary Relevance multi-label classifier\n",
    "# with an SVM classifier\n",
    "\n",
    "classifier = BinaryRelevance(\n",
    "    # Specifying SVM parameters\n",
    "    classifier = SVC(C = 1,                   # Regularization term, by default 1\n",
    "                     kernel = 'rbf',          # Kernell to be used, by default rbf\n",
    "                     gamma = 'scale',         # Gamma parameter in 'rbf' kernell, by default 'scale' = 1 / (n_features * X.var()) \n",
    "                     probability = True,      # Probability estimates for the label predictions p(y_i = 1 | y^_i)\n",
    "                     class_weight ='balanced',# Weights to be used to balance data set. 'balanced' = n_samples / (n_classes * np.bincount(y))\n",
    "                     random_state = 123,      # Controls the pseudo random number generation for shuffling the data for probability estimates.\n",
    "                     cache_size = 1000),      # Specify the size of the kernel cache (in MB). By default 200, increased to 1000 to improve runtime.\n",
    "    require_dense = [False, True]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryRelevance(classifier=SVC(C=1, cache_size=1000, class_weight='balanced',\n",
       "                               probability=True, random_state=123),\n",
       "                require_dense=[False, True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomrance of the fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3192419057646749"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In multi-label classification, \n",
    "# the mean accuracy on the given test data and labels \n",
    "# is the subset accuracy which is a harsh metric since \n",
    "# you require for each sample that each label set be \n",
    "# correctly predicted.\n",
    "classifier.score(X_train, Y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = classifier.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598165656150447"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(Y_train, Y_pred, average = 'micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write probabilities to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = pd.DataFrame(probabilities.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities.to_csv('../output/subtask_1_output', index = False, header = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | C | kernel | gamma | weight | features | F1 score |\n",
    "|---|---|---|---|---|---|---|\n",
    "| run_1 |  1 |  rbf | scale  |  balanced |  median for NA's and mean  | 0.598165656150447 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
