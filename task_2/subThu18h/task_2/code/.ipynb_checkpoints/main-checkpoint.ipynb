{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2, Subtask 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We read the already normalized and imputed data. For specifics about the imputation and normalization \n",
    "## see imputate.R file. \n",
    "test_feat_path = \"../data/test_features_imp.csv\" \n",
    "train_feat_path = \"../data/train_features_imp.csv\" \n",
    "train_lab_path = \"../data/train_labels.csv\"\n",
    "test_feat = pd.read_csv(test_feat_path)\n",
    "train_feat = pd.read_csv(train_feat_path)\n",
    "train_lab = pd.read_csv(train_lab_path)\n",
    "\n",
    "## Order data to make sure that rows in X and Y match\n",
    "test_feat.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "train_feat.sort_values(by=['pid'], inplace = True,ignore_index = True)\n",
    "train_lab.sort_values(by=['pid'], inplace = True, ignore_index = True)\n",
    "\n",
    "## Select exclude the pid column and make into array\n",
    "X_test = test_feat.iloc[:, 1:272].values\n",
    "X_train = train_feat.iloc[:, 1:272].values\n",
    "Y_train = train_lab\n",
    "\n",
    "# Create output file\n",
    "output = pd.DataFrame({'pid': test_feat.iloc[:, 0].values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 1\n",
    "### Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the names of the labels to predict\n",
    "def prob_classsifier(X_train, Y_train, X_test, output):\n",
    "    labels_subtask_1 = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST',\n",
    "                    'LABEL_Alkalinephos', 'LABEL_Bilirubin_total', \n",
    "                    'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "                    'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "    ## Write to an array the labels of interest\n",
    "    Y_train = Y_train[labels_subtask_1].to_numpy()\n",
    "\n",
    "\n",
    "    ## For every label in Y_train fit a HGBC and use it to predict the probabilities of X_test\n",
    "    print(\"ROC AUC validation and training score (training score on probability estimates), for each label:\")\n",
    "    for i, label in enumerate(labels_subtask_1):\n",
    "        ## Fit model\n",
    "        clf = HistGradientBoostingClassifier(scoring = 'roc_auc', \n",
    "                                             random_state = 123).fit(X_train, Y_train[:, i])\n",
    "\n",
    "        ## Print the testing and traing score. Training score is estimated for the probability estimates not the labels.\n",
    "        print(clf.validation_score_[np.size(clf.validation_score_) - 1], \" \", \n",
    "              metrics.roc_auc_score(Y_train[:, i],\n",
    "              clf.predict_proba(X_train)[:, 1], average='micro'))\n",
    "\n",
    "        ## Write to results df\n",
    "        output[label] = clf.predict_proba(X_test)[:, 1]\n",
    "    return output\n",
    "\n",
    "output = prob_classsifier(X_train, Y_train, X_test, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 2\n",
    "### Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(X_train, Y_train, X_test, output):\n",
    "    ## Write to an array the labels of interest\n",
    "    Y_train = Y_train['LABEL_Sepsis'].to_numpy()\n",
    "\n",
    "    ## Fit a HGBC and use it to predict the probabilities of X_test\n",
    "    print(\"ROC AUC validation and training score (training score on probability estimates), for each label:\")\n",
    "\n",
    "    ## Fit model\n",
    "    clf = HistGradientBoostingClassifier(scoring = 'roc_auc',\n",
    "                                         random_state = 123).fit(X_train, Y_train)\n",
    "\n",
    "    ## Print the testing and traing score. Trainig score is estimated for the probability estimates not the labels.\n",
    "    print(clf.validation_score_[np.size(clf.validation_score_) - 1],\n",
    "          \" \",\n",
    "          metrics.roc_auc_score(Y_train,\n",
    "                                clf.predict_proba(X_train)[:, 1],\n",
    "                                average='micro'))\n",
    "\n",
    "    ## Write to results df\n",
    "    output['LABEL_Sepsis'] = clf.predict_proba(X_test)[:, 1]\n",
    "    return output\n",
    "\n",
    "output = classifier(X_train, Y_train, X_test, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtask 3\n",
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def regressor(train_feat, Y_train, test_feat, output):\n",
    "    ## Define the features to predict for this rask\n",
    "    labels_subtask_3 = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "\n",
    "    ## Write to an array the labels of interest\n",
    "    Y_train = Y_train[labels_subtask_3].to_numpy()\n",
    "\n",
    "    ## Fit Lasso regression to the data and predict\n",
    "    print(\"Training scores for each label:\")\n",
    "    for i, label in enumerate(labels_subtask_3):\n",
    "        ## Get suffix of the label to predict\n",
    "        sufix = label.split(\"_\", maxsplit = 2)[1] + \"$\"\n",
    "\n",
    "        ## Filter out columns that dont end with the suffix\n",
    "        X_in_loop_train = train_feat.filter(regex = sufix, axis = 1).to_numpy()\n",
    "        X_in_loop_test = test_feat.filter(regex = sufix, axis = 1).to_numpy()\n",
    "\n",
    "        ## Fit model\n",
    "        reg = LassoCV(random_state = 123, \n",
    "                      verbose = False,\n",
    "                      max_iter = 10000).fit(X_in_loop_train, Y_train[:, i])\n",
    "\n",
    "        ## Print training score (the suck)\n",
    "        print(reg.score(X_in_loop_train, Y_train[:, i]))\n",
    "\n",
    "        ## Write to output\n",
    "        output[label] = reg.predict(X_in_loop_test)\n",
    "    return output\n",
    "\n",
    "output = regressor(train_feat, Y_train, test_feat, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Write results to .zip\n",
    "output.to_csv('submission.zip', index=False, float_format='%.3f', compression='zip')\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the score of our submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VITALS = ['LABEL_RRate', 'LABEL_ABPm', 'LABEL_SpO2', 'LABEL_Heartrate']\n",
    "TESTS = ['LABEL_BaseExcess', 'LABEL_Fibrinogen', 'LABEL_AST', 'LABEL_Alkalinephos', 'LABEL_Bilirubin_total',\n",
    "         'LABEL_Lactate', 'LABEL_TroponinI', 'LABEL_SaO2',\n",
    "         'LABEL_Bilirubin_direct', 'LABEL_EtCO2']\n",
    "\n",
    "\n",
    "def get_score(df_true, df_submission):\n",
    "    df_submission = df_submission.sort_values('pid')\n",
    "    df_true = df_true.sort_values('pid')\n",
    "    task1 = np.mean([metrics.roc_auc_score(df_true[entry], df_submission[entry]) for entry in TESTS])\n",
    "    task2 = metrics.roc_auc_score(df_true['LABEL_Sepsis'], df_submission['LABEL_Sepsis'])\n",
    "    task3 = np.mean([0.5 + 0.5 * np.maximum(0, metrics.r2_score(df_true[entry], df_submission[entry])) for entry in VITALS])\n",
    "    score = np.mean([task1, task2, task3])\n",
    "    print(\"Score task 1: \", task1)\n",
    "    print(\"Score task 2: \", task2)\n",
    "    print(\"Score task 3: \", task3)\n",
    "    scores = [task1, task2, task3, score]\n",
    "    return scores\n",
    "\n",
    "\n",
    "def crossvalidation_analysis(X_cross, y_cross, train_feat):\n",
    "    \"\"\"Cross-validation analysis of our classifiers and regressors\"\"\"\n",
    "    kf = KFold(n_splits=2)\n",
    "    scores = []\n",
    "    for train_index, test_index in kf.split(X_cross):\n",
    "        X_train, X_test = X_cross[train_index], X_cross[test_index]\n",
    "        Y_train, Y_test = y_cross.loc[train_index].reset_index(), y_cross.loc[test_index].reset_index()\n",
    "        X_train_labels, X_test_labels = train_feat.loc[train_index].reset_index(), train_feat.loc[test_index].reset_index()\n",
    "        output = pd.DataFrame({'pid': Y_test.iloc[:, 0].values})\n",
    "        output = prob_classsifier(X_train, Y_train, X_test, output)\n",
    "        output = classifier(X_train, Y_train, X_test, output)\n",
    "        output = regressor(X_train_labels, Y_train, X_test_labels, output)\n",
    "        print(\"Fold score\", get_score(Y_test, output))\n",
    "        scores.append(get_score(Y_test, output))\n",
    "    \n",
    "    scores = pd.DataFrame(scores,columns=['Task1', \"Task2\", \"Task3\", \"Average\"])\n",
    "    print(\"FINAL SCORE: \", np.mean(scores))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "scores = crossvalidation_analysis(X_train, Y_train, train_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 1. Binary Relevance and HGBC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|   | C | kernel | gamma | weight | features | n_features | F1 score | AUC | runtime (min) |\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "| run_1 |  1 |  rbf | scale  |  balanced |  median for NA's and mean  | 35 | 0.598165656150447 | ? | 33 |\n",
    "| run_2 |  1 |  rbf | scale  |  balanced |  median for NA's and mean, max, min, median, sd  | 170 | 0.628216870267411 |?| 102 |\n",
    "| run_3 |  1 |  rbf | scale  |  balanced |  median for NA's and mean, max, min, median, sd, range, skw, kurt  | 272 | 0.649372121402984 | 0.8236937992110356 | 141 |\n",
    "| run_4 |  HGBC |  HGBC | HGBC |  HGBC |  median for NA's and mean, max, min, median, sd, range, skw, kurt  | 272 | 0.871097657278231* | 0.8222653647930391 | 0.5 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*I think the reason for this high score is beacuse the f1_micro is more severe when all labels are taken into account instead of one by one and the averaging. Hence I dont belive the HGBC is superiro in terms of performance, otherwise we would have also observed a big increase in the AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subtask 3. Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainig scores for normalized and unnormalized imputed data restircted to the labels:\n",
    "\n",
    "|nomralized|UN-nomralized|\n",
    "|---|---|\n",
    "|0.37770345083252754 | 0.37759566055685045|\n",
    "|0.5859785441608802  | 0.5856645886174903 |\n",
    "|0.38386074780919743 | 0.3842306307116389 |\n",
    "|0.6144724385873669  | 0.6142282361433877 | \n",
    "\n",
    "The sumbission scores were only a little bit different for normalized and unormalized data. 0.754641671097 and 0.754664968318 respectively. We therofre decide to use normalized data becasue this way we dont need two imputation scripts. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
